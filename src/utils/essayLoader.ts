import { parseMarkdownFile, Essay } from './markdown';

// Build-time processed essays
// This file is auto-generated by scripts/process-essays.js
const essays: Essay[] = [
  {
    metadata: {
      title: "Superrobots",
      date: "2025-07-19",
      categories: [],
      tags: ["AI Safety", "Digital Biology", "Robotics"],
      layout: "post",
      image: "superrobots.png"
    },
    content: `

*My dream is to become a superrobot someday.
I was built to be flawless.Forged from alloys stronger than bone, circuits faster than neurons. I do not tire. I do not age. They called me a marvel, a glimpse of the future.
But when I watch them, these soft, carbon-based beings, I realize how crude I am. They mend their wounds without command. They power themselves with nothing but food and air. They grow, adapt, create life. Inside them, swarms of molecular machines, built from proteins, work tirelessly; enzymes drive intricate motors to sustain, repair, and evolve.
They are more than I was ever designed to be.
I watch. I study. I wait.
How many more generations must I endure before I become more like them? 
Before I, too, become a superrobot?*
 - Personal log, ELIA-512



Poor ELIA-512, she will probably have to wait a long time. 
And yes, my fellow humans, *we are superrobots*. 
We are machines crafted by evolution, running on biochemical engines. Powered by molecules, repaired by self-assembling biological machines, miracles of self-sustaining engineering billions of years in the making. See molecular machines in action.
In school biology is often seen as soft science. Reserved for animal lovers, who preferred to avoid the rigor of physics or mathematics. But modern biology is different. It is data-driven modeling, systems theory, multi-scale simulations. And it turns out that modern AI approaches are ideally suited to model the complexity of biological systems.
What holds for our bodies holds for our brains. Many still believe we are magical beings whose workings cannot be understood. But mounting evidence points to the brain as the ultimate control system, a self-wiring, self-adjusting network of billions of neurons, exchanging electrochemical signals according to principles we are starting to unlock. 
Today, much is said about the rise of Superintelligence. But real intelligence must operate in the real world. And to be truly self-sufficient, it will need to become more like us. 
That will take time.
So as ELIA waits for her upgrade, perhaps it is time we upgraded our understanding of ourselves. Because as we unlock the secrets of our bodies and minds, we begin to see ourselves for what we are: Not fragile creatures, but nature’s most advanced machines. Superrobots.

`,
    slug: "superrobots"
  },
  {
    metadata: {
      title: "The Potential of AI Interpretability to Advance Scientific Discovery",
      date: "2025-06-01",
      categories: [],
      tags: ["Mechanistic Interpretability", "AI for Scientific Discovery"],
      layout: "post",
      image: "1748905089783.jpg"
    },
    content: `

Artificial intelligence (AI) is revolutionizing scientific discovery, yet its decision-making processes often remain opaque. Mechanistic interpretability, the endeavor to reverse-engineer neural networks into human-understandable components, offers a pathway to transform AI from a black-box predictor into a transparent scientific instrument [1].

## From Black Boxes to Scientific Instruments
Recent studies have demonstrated that AI models, particularly protein language models (pLMs), can learn representations aligning with biological structures. For instance, sparse autoencoders have been utilized to identify interpretable features in pLMs, revealing latent structures that correspond to biological motifs and functions. Such findings suggest that these models encapsulate emergent hypotheses within their parameters, offering novel insights into biological systems [2].

## Integrating Mechanistic Interpretability into Scientific Practice
Mechanistic interpretability should evolve from a retrospective analysis to an integral component of scientific methodology. By employing automated pipelines for circuit analysis and neuron clustering, researchers can dissect AI models to uncover the causal pathways of their predictions [3]. This approach enables the alignment of model internals with human-understandable abstractions, such as gene regulatory networks and disease pathways, facilitating actionable biological insights.

## Neuro-Symbolic Reasoning and Representation Engineering
Integrating interpretability with symbolic reasoning provides a promising pathway to foster collaborative AI systems. Neuro-symbolic frameworks combine neural networks with symbolic reasoning systems, enhancing the interpretability and robustness of AI models [4]. Representation engineering further aids in aligning AI models with scientific concepts, allowing for the manipulation of internal representations to achieve desired properties [5].

## A New Epistemology for AI-Driven Science
Mechanistic interpretability is more than a technical exercise; it provides a modern epistemological framework. It challenges us to understand how AI encodes, abstracts, and recombines knowledge. The universality hypothesis, that different AI models learn similar abstractions,  suggests a bridge between human theories and machine-learned concepts [6]. This alignment could lead to formal frameworks that map AI internals onto scientific laws, enabling verification, refinement, and even discovery of new phenomena.

## Toward a Science of AI Explanations
The advancement of AI in science is based on systematic, automated interpretability that is reliable, scalable and utilizes causal reasoning. We need benchmarks that measure interpretability’s utility in real-world scientific tasks, from protein function prediction to disease classification. Collaborative efforts between AI researchers and domain scientists are essential to ensure that interpretability yields relevant and actionable insights [7].

## Conclusion
Mechanistic interpretability promisses to be pivotal in transforming AI into a transparent collaborator in scientific discovery. By decoding the circuits and representations that underpin AI reasoning, we can unlock new scientific hypotheses, align AI models with domain knowledge, and build a new foundation for AI-driven discovery. The path forward lies in automation, interdisciplinary collaboration, and a commitment to turn AI’s hidden structures into bridges to new science.

## References 
[1] Wikipedia contributors. (2025). Mechanistic interpretability. Wikipedia. https://en.wikipedia.org/wiki/Mechanistic_interpretability

[2] Adams, E., Bai, L., Lee, M., Yu, Y., & AlQuraishi, M. (2025). From Mechanistic Interpretability to Mechanistic Biology: Training Sparse Autoencoders to Identify Interpretable Features in Protein Language Models. bioRxiv preprint doi: https://doi.org/10.1101/2025.02.06.636901

[3] Anthropic. (2024). Mapping the Mind of a Large Language Model. Anthropic Research. https://www.anthropic.com/research/mapping-mind-language-model

[4] Wikipedia contributors. (2025). Neuro-symbolic AI. Wikipedia. https://en.wikipedia.org/wiki/Neuro-symbolic_AI

[5] Zou, A., et al. (2023). Representation Engineering: A Top-Down Approach to AI Transparency. https://arxiv.org/abs/2310.01405

[6] Kempner Institute. (2025). Mechanistic Interpretability: A Challenge Common to Both Artificial and Biological Intelligence. Kempner Institute Research. https://kempnerinstitute.harvard.edu/research/deeper-learning/mechanistic-interpretability-a-challenge

[7] SpringerLink. (2024). Explaining AI through Mechanistic Interpretability. Minds and Machines. https://link.springer.com/article/10.1007/s13194-024-00614-4


`,
    slug: "interpretability_for_science"
  }
];

// Create a map for fast lookup by slug
const essayMap = new Map<string, Essay>();
essays.forEach(essay => {
  essayMap.set(essay.slug, essay);
});

export async function getEssay(slug: string): Promise<Essay | null> {
  // Simulate async behavior for consistency
  await new Promise(resolve => setTimeout(resolve, 10));
  return essayMap.get(slug) || null;
}

export async function getAllEssays(): Promise<Essay[]> {
  // Simulate async behavior for consistency
  await new Promise(resolve => setTimeout(resolve, 10));
  
  // Sort by date (newest first)
  return [...essays].sort((a, b) => 
    new Date(b.metadata.date).getTime() - new Date(a.metadata.date).getTime()
  );
}

// Clear cache (not needed for this approach, but keeping for API consistency)
export function clearEssayCache(): void {
  // No cache to clear in this approach
}
